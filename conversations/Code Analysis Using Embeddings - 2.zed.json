{"id":"5ba16359-7834-47d1-8535-f1b36de8679f","zed":"context","version":"0.4.0","text":"\n\n\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nproject_path = \"./repo_clone\"\n\ndef analyze_single_file(file_path: str) -> str:\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            code_content = f.read()\n\n        response = client.chat.completions.create(\n            model=\"llama-3.1-8b-lexi-uncensored-v2\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You're a senior code analyst. Analyze the provided code and describe:\\n\"\n                        \"1. Основное назначение файла\\n\"\n                        \"2. Ключевые функции/классы\\n\"\n                        \"3. Входные/выходные данные (если применимо)\\n\"\n                        \"4. Зависимости (если видны)\\n\"\n                        \"5. Особенности реализации\\n\"\n                        \"Будь лаконичным (3-5 предложений).\\n\"\n                        \"Отвечай на русском языке.\"\n                    ),\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Проанализируй этот код:\\n\\n{code_content}\",\n                },\n            ],\n            temperature=0.2,\n        )\n\n        if response.choices and len(response.choices) > 0:\n            return response.choices[0].message.content.strip()\n        return \"Не удалось проанализировать код\"\n\n    except Exception as e:\n        return f\"Ошибка при анализе файла {file_path}: {str(e)}\"\n\ndef analyze_code_project(repo_path: str) -> str:\n    allowed_extensions = (\n        \".py\", \".md\", \".txt\", \".js\", \".java\", \n        \".cpp\", \".h\", \".html\", \".css\",\n    )\n    \n    analysis_results = []\n    total_files = 0\n    analyzed_files = 0\n    \n    for root, dirs, files in os.walk(repo_path):\n        if \".git\" in dirs:\n            dirs.remove(\".git\")\n\n        for file in files:\n            if not file.lower().endswith(allowed_extensions):\n                continue\n            total_files += 1\n            file_path = os.path.join(root, file)\n            \n            # Анализируем каждый файл отдельно\n            file_analysis = analyze_single_file(file_path)\n            analysis_results.append(f\"Файл: {file_path}\\nАнализ: {file_analysis}\\n\")\n            analyzed_files += 1\n    \n    if not analysis_results:\n        return \"В проекте не найдено файлов для анализа.\"\n    \n    # Создаем суммарный отчет\n    summary = (\n        f\"Общий анализ проекта: {repo_path}\\n\"\n        f\"Всего файлов: {total_files}\\n\"\n        f\"Проанализировано файлов: {analyzed_files}\\n\\n\"\n        \"Детальный анализ по файлам:\\n\\n\" + \n        \"\\n\".join(analysis_results)\n    )\n    \n    return summary\n\nif __name__ == \"__main__\":\n    analysis_result = analyze_code_project(project_path)\n    print(\"\\nРезультат анализа проекта:\")\n    print(analysis_result)\n\nВместо того, чтобы модель давала отчет по каждому файлу я хочу использоовать имбединги.\nfrom sentence_transformers import SentenceTransformer\n\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nembeddings = embedding_model.encode([\"your code chunk here\"])\n\n\n<think>\nYou want to modify your code to use embeddings for analyzing code files instead of having the model generate a full report for each file. Let's modify your code to incorporate sentence embeddings using the `sentence_transformers` library.\n\nHere's how we can adapt your code:\n\n1. We'll use embeddings to represent each code file\n2. We'll store these embeddings along with file metadata\n3. We can then use these embeddings to:\n   - Find similar files\n   - Cluster files by similarity\n   - Generate a summary of the overall project structure\n\nLet's rewrite your code:\n\n```python\nfrom openai import OpenAI\nimport os\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom typing import List, Dict, Any\nimport json\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nproject_path = \"./repo_clone\"\n\n# Initialize the embedding model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\ndef generate_code_embedding(code_content: str):\n    \"\"\"Generate an embedding for a code snippet.\"\"\"\n    embedding = embedding_model.encode([code_content])[0]\n    return embedding.tolist()  # Convert to list for JSON serialization\n\ndef analyze_single_file(file_path: str) -> Dict[str, Any]:\n    \"\"\"Analyze a single file and return its metadata and embedding.\"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            code_content = f.read()\n        \n        # Generate embedding for the code content\n        embedding = generate_code_embedding(code_content)\n        \n        # Extract basic file metadata\n        file_info = {\n            \"path\": file_path,\n            \"extension\": os.path.splitext(file_path)[1],\n            \"size_bytes\": os.path.getsize(file_path),\n            \"last_modified\": os.path.getmtime(file_path),\n            \"embedding\": embedding,\n            \"content_preview\": code_content[:200] + \"...\" if len(code_content) > 200 else code_content\n        }\n        \n        return file_info\n        \n    except Exception as e:\n        return {\n            \"path\": file_path,\n            \"error\": str(e),\n            \"embedding\": []\n        }\n\ndef analyze_code_project(repo_path: str) -> Dict[str, Any]:\n    \"\"\"Analyze the entire code project using embeddings.\"\"\"\n    allowed_extensions = (\n        \".py\", \".md\", \".txt\", \".js\", \".java\", \n        \".cpp\", \".h\", \".html\", \".css\",\n    )\n    \n    file_analyses = []\n    total_files = 0\n    analyzed_files = 0\n    \n    for root, dirs, files in os.walk(repo_path):\n        if \".git\" in dirs:\n            dirs.remove(\".git\")\n\n        for file in files:\n            if not file.lower().endswith(allowed_extensions):\n                continue\n            total_files += 1\n            file_path = os.path.join(root, file)\n            \n            # Analyze each file and get its embedding\n            file_analysis = analyze_single_file(file_path)\n            file_analyses.append(file_analysis)\n            analyzed_files += 1\n    \n    # Create project summary\n    project_data = {\n        \"project_path\": repo_path,\n        \"total_files\": total_files,\n        \"analyzed_files\": analyzed_files,\n        \"file_analyses\": file_analyses\n    }\n    \n    return project_data\n\ndef find_similar_files(project_data: Dict[str, Any], query_file_index: int, top_n: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"Find the most similar files to a query file based on embedding similarity.\"\"\"\n    files = project_data[\"file_analyses\"]\n    query_embedding = np.array(files[query_file_index][\"embedding\"])\n    \n    similarities = []\n    for i, file_data in enumerate(files):\n        if i == query_file_index or not file_data.get(\"embedding\"):\n            continue\n        \n        file_embedding = np.array(file_data[\"embedding\"])\n        # Compute cosine similarity\n        similarity = np.dot(query_embedding, file_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(file_embedding))\n        similarities.append((i, similarity))\n    \n    # Sort by similarity (descending)\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    \n    # Return top N similar files\n    return [(files[i], sim) for i, sim in similarities[:top_n]]\n\ndef generate_project_summary(project_data: Dict[str, Any]) -> str:\n    \"\"\"Generate a summary of the project based on the collected data and embeddings.\"\"\"\n    # Here we can use an LLM to generate a summary based on the project structure\n    file_paths = [f[\"path\"] for f in project_data[\"file_analyses\"]]\n    file_types = {}\n    \n    for file in project_data[\"file_analyses\"]:\n        ext = file.get(\"extension\", \"unknown\")\n        if ext in file_types:\n            file_types[ext] += 1\n        else:\n            file_types[ext] = 1\n    \n    # Prepare a prompt for the LLM\n    file_list = \"\\n\".join(file_paths)\n    prompt = f\"\"\"\n    I have a code project with the following structure:\n    {file_list}\n    \n    Project statistics:\n    - Total files: {project_data['total_files']}\n    - File types: {json.dumps(file_types, indent=2)}\n    \n    Please provide a brief summary of what this project might be about, based on its structure.\n    Answer in Russian language.\n    \"\"\"\n    \n    # Get the summary from the LLM\n    response = client.chat.completions.create(\n        model=\"llama-3.1-8b-lexi-uncensored-v2\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You're a senior code analyst who analyzes project structures.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            },\n        ],\n        temperature=0.2,\n    )\n    \n    if response.choices and len(response.choices) > 0:\n        return response.choices[0].message.content.strip()\n    return \"Не удалось создать общий анализ проекта\"\n\nif __name__ == \"__main__\":\n    # Analyze the project and get embeddings\n    project_data = analyze_code_project(project_path)\n    \n    # Save the project data\n    with open(\"project_analysis.json\", \"w\") as f:\n        # Remove the embeddings to make the file smaller\n        project_data_save = {k: v for k, v in project_data.items() if k != \"file_analyses\"}\n        project_data_save[\"file_analyses\"] = []\n        for file_analysis in project_data[\"file_analyses\"]:\n            file_analysis_save = {k: v for k, v in file_analysis.items() if k != \"embedding\"}\n            project_data_save[\"file_analyses\"].append(file_analysis_save)\n        json.dump(project_data_save, f, indent=2)\n    \n    # Generate a summary\n    summary = generate_project_summary(project_data)\n    print(\"\\nРезультат анализа проекта:\")\n    print(summary)\n    \n    # Example of finding similar files\n    if project_data[\"file_analyses\"]:\n        print(\"\\nПример поиска похожих файлов:\")\n        query_file_index = 0\n        similar_files = find_similar_files(project_data, query_file_index, top_n=3)\n        print(f\"Файлы, похожие на {project_data['file_analyses'][query_file_index]['path']}:\")\n        for file, similarity in similar_files:\n            print(f\"- {file['path']} (Сходство: {similarity:.4f})\")\n```\n\nThis revised code:\n\n1. Generates embeddings for each code file using the SentenceTransformer model\n2. Stores file metadata along with the embeddings\n3. Provides a function to find similar files based on embedding similarity\n4. Generates a project summary by analyzing the file structure (without needing to analyze each file's code)\n5. Saves the analysis results to a JSON file (excluding embeddings to keep the file size reasonable)\n\nYou can extend this further by:\n1. Clustering similar files together\n2. Visualizing the relationships between files\n3. Building a search interface to query the code base\n4. Adding more sophisticated analysis of the embeddings\n\nLet me know if you need further adjustments or have specific requirements for how you want to use the embeddings!\n</think>\n\nHere's how you can enhance your code analysis tool with embeddings to make it more sophisticated:\n\n```python\nfrom openai import OpenAI\nimport os\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom typing import List, Dict, Any\nimport json\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nproject_path = \"./repo_clone\"\n\n# Инициализируем модель для эмбеддингов\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\ndef create_file_embedding(file_path: str) -> Dict[str, Any]:\n    \"\"\"Создает эмбеддинг для файла и возвращает метаданные.\"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            code_content = f.read()\n        \n        # Создаем эмбеддинг для содержимого файла\n        embedding = embedding_model.encode([code_content])[0]\n        \n        return {\n            \"path\": file_path,\n            \"extension\": os.path.splitext(file_path)[1],\n            \"size_bytes\": os.path.getsize(file_path),\n            \"embedding\": embedding.tolist(),\n            \"content_preview\": code_content[:200] + \"...\" if len(code_content) > 200 else code_content\n        }\n    except Exception as e:\n        return {\n            \"path\": file_path,\n            \"error\": str(e),\n            \"embedding\": []\n        }\n\ndef analyze_code_project(repo_path: str) -> Dict[str, Any]:\n    \"\"\"Анализирует проект с использованием эмбеддингов.\"\"\"\n    allowed_extensions = (\n        \".py\", \".md\", \".txt\", \".js\", \".java\", \n        \".cpp\", \".h\", \".html\", \".css\",\n    )\n    \n    file_data = []\n    total_files = 0\n    analyzed_files = 0\n    \n    for root, dirs, files in os.walk(repo_path):\n        if \".git\" in dirs:\n            dirs.remove(\".git\")\n\n        for file in files:\n            if not file.lower().endswith(allowed_extensions):\n                continue\n            total_files += 1\n            file_path = os.path.join(root, file)\n            \n            # Создаем эмбеддинг для файла\n            file_info = create_file_embedding(file_path)\n            file_data.append(file_info)\n            analyzed_files += 1\n    \n    return {\n        \"project_path\": repo_path,\n        \"total_files\": total_files,\n        \"analyzed_files\": analyzed_files,\n        \"file_data\": file_data\n    }\n\ndef find_similar_files(project_data: Dict[str, Any], query_path: str, top_n: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"Находит похожие файлы на основе близости эмбеддингов.\"\"\"\n    files = project_data[\"file_data\"]\n    \n    # Находим индекс запрашиваемого файла\n    query_index = None\n    for i, file in enumerate(files):\n        if file[\"path\"] == query_path:\n            query_index = i\n            break\n    \n    if query_index is None:\n        return []\n    \n    query_embedding = np.array(files[query_index][\"embedding\"])\n    similarities = []\n    \n    for i, file in enumerate(files):\n        if i == query_index or not file.get(\"embedding\"):\n            continue\n        \n        file_embedding = np.array(file[\"embedding\"])\n        # Вычисляем косинусное сходство\n        similarity = np.dot(query_embedding, file_embedding) / (\n            np.linalg.norm(query_embedding) * np.linalg.norm(file_embedding)\n        )\n        similarities.append((i, similarity))\n    \n    # Сортируем по убыванию сходства\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    \n    # Возвращаем топ N похожих файлов\n    similar_files = []\n    for i, sim in similarities[:top_n]:\n        similar_files.append({\n            \"path\": files[i][\"path\"],\n            \"similarity\": sim,\n            \"preview\": files[i].get(\"content_preview\", \"\")\n        })\n    \n    return similar_files\n\ndef cluster_files(project_data: Dict[str, Any], n_clusters: int = 5) -> Dict[str, List[str]]:\n    \"\"\"Группирует файлы в кластеры на основе эмбеддингов.\"\"\"\n    valid_files = []\n    embeddings = []\n    \n    for file in project_data[\"file_data\"]:\n        if file.get(\"embedding\") and len(file[\"embedding\"]) > 0:\n            valid_files.append(file)\n            embeddings.append(file[\"embedding\"])\n    \n    if not embeddings:\n        return {\"error\": \"Нет валидных эмбеддингов для кластеризации\"}\n    \n    # Применяем KMeans для кластеризации\n    kmeans = KMeans(n_clusters=min(n_clusters, len(embeddings)), random_state=42)\n    clusters = kmeans.fit_predict(embeddings)\n    \n    # Группируем файлы по кластерам\n    grouped_files = {}\n    for i, cluster_id in enumerate(clusters):\n        cluster_id = int(cluster_id)\n        if cluster_id not in grouped_files:\n            grouped_files[cluster_id] = []\n        grouped_files[cluster_id].append(valid_files[i][\"path\"])\n    \n    return grouped_files\n\ndef generate_project_summary(project_data: Dict[str, Any], clusters: Dict[str, List[str]]) -> str:\n    \"\"\"Генерирует общий анализ проекта на основе структуры и кластеров.\"\"\"\n    file_types = {}\n    for file in project_data[\"file_data\"]:\n        ext = file.get(\"extension\", \"unknown\")\n        if ext in file_types:\n            file_types[ext] += 1\n        else:\n            file_types[ext] = 1\n    \n    # Формируем информацию о кластерах для LLM\n    cluster_info = \"\"\n    for cluster_id, files in clusters.items():\n        cluster_info += f\"Кластер {cluster_id}:\\n\"\n        for f in files[:5]:  # Показываем только первые 5 файлов из кластера\n            cluster_info += f\"- {f}\\n\"\n        if len(files) > 5:\n            cluster_info += f\"... и еще {len(files) - 5} файлов\\n\"\n    \n    # Создаем промпт для LLM\n    prompt = f\"\"\"\n    Проанализируй структуру кодового проекта:\n    \n    Статистика проекта:\n    - Всего файлов: {project_data['total_files']}\n    - Типы файлов: {json.dumps(file_types, indent=2)}\n    \n    Кластеры похожих файлов:\n    {cluster_info}\n    \n    Опиши, о чем может быть этот проект, его основную архитектуру и назначение.\n    Отвечай на русском языке, будь лаконичным (5-7 предложений).\n    \"\"\"\n    \n    # Получаем суммарный анализ от LLM\n    response = client.chat.completions.create(\n        model=\"llama-3.1-8b-lexi-uncensored-v2\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Ты старший аналитик кода, который анализирует структуру проектов.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            },\n        ],\n        temperature=0.2,\n    )\n    \n    if response.choices and len(response.choices) > 0:\n        return response.choices[0].message.content.strip()\n    return \"Не удалось создать общий анализ проекта\"\n\nif __name__ == \"__main__\":\n    # Анализируем проект и создаем эмбеддинги\n    print(\"Анализ проекта и создание эмбеддингов...\")\n    project_data = analyze_code_project(project_path)\n    \n    # Кластеризуем файлы\n    print(\"Кластеризация файлов...\")\n    clusters = cluster_files(project_data, n_clusters=5)\n    \n    # Генерируем общий анализ проекта\n    print(\"Создание общего анализа проекта...\")\n    summary = generate_project_summary(project_data, clusters)\n    \n    print(\"\\nРезультат анализа проекта:\")\n    print(summary)\n    \n    # Пример поиска похожих файлов\n    if project_data[\"file_data\"]:\n        print(\"\\nПример поиска похожих файлов:\")\n        example_file = project_data[\"file_data\"][0][\"path\"]\n        similar_files = find_similar_files(project_data, example_file, top_n=3)\n        print(f\"Файлы, похожие на {example_file}:\")\n        for file in similar_files:\n            print(f\"- {file['path']} (Сходство: {file['similarity']:.4f})\")\n    \n    # Сохраняем результаты анализа\n    with open(\"project_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n        # Удаляем эмбеддинги для экономии места\n        save_data = {k: v for k, v in project_data.items() if k != \"file_data\"}\n        save_data[\"file_data\"] = []\n        for file_info in project_data[\"file_data\"]:\n            file_info_save = {k: v for k, v in file_info.items() if k != \"embedding\"}\n            save_data[\"file_data\"].append(file_info_save)\n        json.dump(save_data, f, indent=2, ensure_ascii=False)\n    \n    print(\"\\nАнализ сохранен в файл project_analysis.json\")\n```\n\nЭтот код:\n\n1. Создает эмбеддинги для всех файлов проекта с помощью SentenceTransformer\n2. Кластеризует файлы на основе сходства их эмбеддингов (находит похожие файлы)\n3. Ищет файлы, похожие на запрошенный, используя косинусное сходство\n4. Генерирует общий анализ проекта на основе структуры и кластеров\n5. Сохраняет результаты анализа в JSON-файл\n\nПреимущества этого подхода:\n- Понимание структуры проекта через анализ схожести файлов\n- Возможность поиска похожих файлов\n- Выявление кластеров связанного кода\n- Более глубокий анализ кодовой базы\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":549}}},{"id":{"replica_id":0,"value":8},"start":3293,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":547}}},{"id":{"replica_id":0,"value":9},"start":3294,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":548}}},{"id":{"replica_id":0,"value":114},"start":3455,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":550}}},{"id":{"replica_id":0,"value":125},"start":3630,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":543}}},{"id":{"replica_id":0,"value":109},"start":3631,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":544}}},{"id":{"replica_id":0,"value":138},"start":3632,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":545}}},{"id":{"replica_id":0,"value":139},"start":21846,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":546}}}],"summary":"Code Analysis Using Embeddings","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}],"thought_process_output_sections":[{"range":{"start":3632,"end":11540}}]}