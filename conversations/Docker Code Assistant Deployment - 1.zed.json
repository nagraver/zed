{"id":"ddb60510-942f-41e9-a9ba-d6e43cb88762","zed":"context","version":"0.4.0","text":"\n\n\nfrom pathlib import Path\nimport os\nimport requests\nimport gradio as gr\nimport logging\nfrom typing import Optional, List\nfrom uuid import uuid4\nimport subprocess\nimport tempfile\n\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_text_splitters import SentenceTransformersTokenTextSplitter\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.documents import Document\nfrom langchain_qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import Distance, VectorParams\n\n# Настройка логирования\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nCOLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"code-assistant-store\")\n\n\nclass LMStudioLLM(LLM):\n    model: str = \"local-model\"\n    base_url: str = \"http://127.0.0.1:1234/v1\"\n    temperature: float = 0.7\n    max_tokens: int = 1024\n    top_p: float = 0.9\n    frequency_penalty: float = 0.0\n    presence_penalty: float = 0.0\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        payload = {\n            \"model\": self.model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            \"top_p\": self.top_p,\n            \"frequency_penalty\": self.frequency_penalty,\n            \"presence_penalty\": self.presence_penalty,\n        }\n        response = requests.post(f\"{self.base_url}/chat/completions\", json=payload)\n        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n\n    @property\n    def _llm_type(self) -> str:\n        return \"lmstudio-llm\"\n\n\ndef configure_qdrant() -> QdrantVectorStore:\n    \"\"\"Configure and return a Qdrant vector store.\"\"\"\n    logger.info(\"Configuring Qdrant client and vector store...\")\n\n    # Инициализация модели эмбедингов\n    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\", encode_kwargs={\"truncation\": True, \"max_length\": 512})\n    embeddings.embed_query(\"test\")  # Прогрев модели\n\n    # Создание клиента Qdrant\n    client = QdrantClient(path=\"/tmp/langchain_qdrant\")\n\n    # Создание коллекции, если она не существует\n    try:\n        client.create_collection(\n            collection_name=COLLECTION_NAME,\n            vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n        )\n    except ValueError:\n        logger.info(\"Collection already exists. Skipping creation.\")\n\n    # Создание векторного хранилища\n    vector_store = QdrantVectorStore(\n        client=client,\n        collection_name=COLLECTION_NAME,\n        embedding=embeddings,\n    )\n\n    logger.info(\"Qdrant configuration completed.\")\n    return vector_store\n\n\ndef clone_github_repo(repo_url: str, temp_dir: str) -> str:\n    \"\"\"Clone GitHub repository to temporary directory.\"\"\"\n    try:\n        repo_name = repo_url.split(\"/\")[-1]\n        if repo_name.endswith(\".git\"):\n            repo_name = repo_name[:-4]\n        repo_path = os.path.join(temp_dir, repo_name)\n\n        subprocess.run([\"git\", \"clone\", repo_url, repo_path], check=True)\n        logger.info(f\"Repository cloned successfully to {repo_path}\")\n        return repo_path\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Failed to clone repository: {e}\")\n        raise\n\n\ndef load_documents_from_repo(repo_path: str) -> List[Document]:\n    \"\"\"Load documents from repository.\"\"\"\n    documents = []\n\n    for root, _, files in os.walk(repo_path):\n        for file in files:\n            if file.endswith((\".txt\", \".md\", \".py\", \".js\", \".java\", \".cpp\", \".h\", \".html\", \".css\")):\n                file_path = os.path.join(root, file)\n                try:\n                    loader = TextLoader(file_path, encoding=\"utf-8\")\n                    loaded_docs = loader.load()\n                    for doc in loaded_docs:\n                        doc.metadata[\"source_file\"] = file_path  # Добавляем путь к файлу в метаданные\n                    documents.extend(loaded_docs)\n                    logger.info(f\"Loaded {file_path}\")\n                except Exception as e:\n                    logger.warning(f\"Could not load {file_path}: {e}\")\n\n    return documents\n\n\ndef add_repository_to_store(\n    vector_store: QdrantVectorStore, repo_url: str, chunk_size: int = 512, chunk_overlap: int = 50\n) -> None:\n    \"\"\"Add repository documents to vector store.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        try:\n            # Клонируем репозиторий\n            repo_path = clone_github_repo(repo_url, temp_dir)\n\n            # Загружаем документы\n            documents = load_documents_from_repo(repo_path)\n\n            if not documents:\n                raise ValueError(\"No documents found in repository\")\n\n            # Разделяем документы на чанки\n            splitter = SentenceTransformersTokenTextSplitter(\n                tokens_per_chunk=chunk_size, chunk_overlap=chunk_overlap, model_name=\"BAAI/bge-m3\"\n            )\n            texts = splitter.split_documents(documents)\n\n            # Добавляем в векторное хранилище\n            ids = [str(uuid4()) for _ in texts]\n            vector_store.add_documents(documents=texts, ids=ids)\n            logger.info(f\"Added {len(texts)} documents from repository to vector store\")\n\n        except Exception as e:\n            logger.error(f\"Error processing repository: {e}\")\n            raise\n\n\ndef load_or_create_vector_store(\n    vector_store: QdrantVectorStore, file_path: str = None, chunk_size: int = 512, chunk_overlap: int = 50\n) -> QdrantVectorStore:\n    \"\"\"Load or create vector store with documents.\"\"\"\n    # Проверяем, есть ли уже документы в хранилище\n    client = vector_store.client\n    collection_info = client.get_collection(COLLECTION_NAME)\n\n    if collection_info.vectors_count == 0 and file_path:\n        logger.info(\"No documents in vector store. Creating new index...\")\n\n        # Загрузка документов\n        loader = TextLoader(file_path)\n        documents = loader.load()\n        for doc in documents:\n            doc.metadata[\"source_file\"] = file_path  # Добавляем путь к файлу в метаданные\n\n        # Разделение документов на чанки\n        splitter = SentenceTransformersTokenTextSplitter(\n            tokens_per_chunk=chunk_size, chunk_overlap=chunk_overlap, model_name=\"BAAI/bge-m3\"\n        )\n        texts = splitter.split_documents(documents)\n\n        # Добавление документов в хранилище\n        ids = [str(uuid4()) for _ in texts]\n        vector_store.add_documents(documents=texts, ids=ids)\n        logger.info(f\"Added {len(texts)} documents to vector store.\")\n    else:\n        logger.info(f\"Vector store already contains {collection_info.vectors_count} vectors. Using existing index.\")\n\n    return vector_store\n\n\n# === Инициализация компонентов ===\nllm = LMStudioLLM()\nvector_store = configure_qdrant()\n\n# Загрузка или создание векторного хранилища\nFILE_PATH = r\"./my_notes.txt\"\nif os.path.exists(FILE_PATH):\n    vector_store = load_or_create_vector_store(vector_store, FILE_PATH)\nelse:\n    vector_store = load_or_create_vector_store(vector_store)\n\n\n# === Функции для Gradio ===\ndef answer_question(question, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda):\n    # Обновляем параметры LLM\n    llm.temperature = temperature\n    llm.max_tokens = max_tokens\n    llm.top_p = top_p\n\n    # Настраиваем ретривер с новыми параметрами\n    retriever = vector_store.as_retriever(\n        search_type=\"mmr\", search_kwargs={\"k\": k_results, \"fetch_k\": fetch_k, \"lambda_mult\": mmr_lambda}\n    )\n\n    # Сборка цепочки с текущими параметрами\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        retriever=retriever,\n        chain_type=\"stuff\",\n        chain_type_kwargs={\"prompt\": prompt_template},\n        return_source_documents=True,\n    )\n\n    response = qa_chain({\"query\": question})\n\n    # Получаем ответ и исходные документы\n    answer = response[\"result\"]\n    source_docs = response[\"source_documents\"]\n\n    # Формируем список уникальных исходных файлов\n    source_files = set()\n    for doc in source_docs:\n        if \"source_file\" in doc.metadata:\n            source_files.add(doc.metadata[\"source_file\"])\n\n    # Формируем итоговый ответ с указанием источников\n    if source_files:\n        answer += \"\\n\\nИсточники:\\n- \" + \"\\n- \".join(source_files)\n\n    return answer\n\n\ndef add_repository(repo_url, chunk_size, chunk_overlap):\n    try:\n        add_repository_to_store(vector_store, repo_url, chunk_size, chunk_overlap)\n        return \"Репозиторий успешно добавлен!\"\n    except Exception as e:\n        return f\"Ошибка при добавлении репозитория: {str(e)}\"\n\n\n# === Кастомный промпт ===\nprompt_template = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=(\n        \"Твоя задача - анализировать загруженный проект и отвечать на вопросы пользователя касательно проекта\\n\\n\"\n        \"Контекст:\\n{context}\\n\\nВопрос: {question}\\nОтвет:\"\n    ),\n)\n\n# === Интерфейс Gradio ===\ncss_file = Path(\"./gradio_style.css\")\n\nwith gr.Blocks(css=css_file.read_text()) as demo:\n    with gr.Tabs():\n        with gr.Tab(\"Добавить репозиторий\", id=\"repo_tab\"):\n            with gr.Row():\n                with gr.Column():\n                    repo_url_input = gr.Textbox(\n                        placeholder=\"https://github.com/username/repository.git\", label=\"URL GitHub репозитория\"\n                    )\n                    with gr.Accordion(\"Настройки чанкинга\", open=False):\n                        chunk_size = gr.Slider(128, 1024, value=512, step=64, label=\"Размер чанка (токены)\")\n                        chunk_overlap = gr.Slider(0, 256, value=50, step=16, label=\"Перекрытие чанков (токены)\")\n                    add_repo_button = gr.Button(\"Добавить репозиторий\", variant=\"primary\")\n                with gr.Column():\n                    repo_status_output = gr.Textbox(label=\"Статус\", interactive=False)\n\n        with gr.Tab(\"Задать вопрос\", id=\"question_tab\"):\n            with gr.Row():\n                with gr.Column():\n                    question_input = gr.Textbox(lines=2, placeholder=\"Задай вопрос по проекту...\", label=\"Вопрос\")\n                    with gr.Accordion(\"Настройки модели\", open=False):\n                        temperature = gr.Slider(0.1, 2.0, value=0.7, step=0.1, label=\"Temperature\")\n                        max_tokens = gr.Slider(128, 4096, value=1024, step=128, label=\"Max Tokens\")\n                        top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.1, label=\"Top-p\")\n                    with gr.Accordion(\"Настройки поиска\", open=False):\n                        k_results = gr.Slider(1, 10, value=5, step=1, label=\"Количество результатов\")\n                        fetch_k = gr.Slider(5, 50, value=20, step=5, label=\"Fetch K (кол-во кандидатов)\")\n                        mmr_lambda = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label=\"MMR Lambda (разнообразие)\")\n                    ask_button = gr.Button(\"Спросить\", variant=\"primary\")\n                with gr.Column():\n                    answer_output = gr.Textbox(label=\"Ответ\", interactive=False)\n\n        with gr.Tab(\"Информация\", id=\"info_tab\"):\n            gr.Markdown(\"### Инструкция по использованию\")\n            gr.Markdown(\n                \"\"\"\n            1. **Добавить репозиторий** - загрузите новый репозиторий для анализа\n            2. **Задать вопрос** - задавайте вопросы о вашем проекте\n            3. Настройте параметры модели и поиска для лучших результатов\n            \"\"\"\n            )\n            gr.Markdown(\"### Технологии\")\n            gr.Markdown(\n                \"\"\"\n            - **LM Studio** - локальный LLM\n            - **LangChain** - обработка и анализ текста\n            - **Qdrant** - векторное хранилище\n            - **bge-m3** - модели эмбедингов\n            \"\"\"\n            )\n\n    ask_button.click(\n        answer_question,\n        inputs=[question_input, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda],\n        outputs=answer_output,\n    )\n    add_repo_button.click(\n        add_repository, inputs=[repo_url_input, chunk_size, chunk_overlap], outputs=repo_status_output\n    )\n\nif __name__ == \"__main__\":\n    # Запуск с автоматическим созданием временной общедоступной ссылки\n    demo.launch(\n        server_name=\"0.0.0.0\",\n        server_port=7860,\n        share=True,  # Это создаст временную общедоступную ссылку\n        show_error=True,\n        debug=True\n    )\n\nЯ бы хотел развернуть это на своем сервере в контейнере Docker. Но я полагаю, что мощности стандартного впс сервера будет недостаточно\n\n<think>\nLet's analyze what you have here and what you want to do:\n\n1. You have a Python application that serves as a code assistant. It uses:\n   - LangChain and Hugging Face for text processing and embeddings\n   - Qdrant for vector storage\n   - LM Studio as a local LLM (large language model)\n   - Gradio for the web interface\n\n2. You want to deploy this in a Docker container on a VPS (Virtual Private Server), but you're concerned about the server's computing power.\n\nLet's address your concerns and provide a solution for containerizing this application:\n\n### Hardware Requirements Analysis\n\nThe most resource-intensive components of your application are:\n1. **LM Studio's LLM** - This will require significant GPU or CPU resources depending on the model\n2. **Vector embeddings** - The `BAAI/bge-m3` model requires moderate resources\n3. **Qdrant Vector Store** - Storage and search (scales with data size)\n\n### Dockerization Approach\n\nLet's create a Dockerfile that sets up your environment. I'll also suggest a couple of optimization options.\n\nHere's what we need:\n1. A Dockerfile\n2. A docker-compose file to manage services\n3. Possibly a configuration for more efficient model usage\n\n### Recommended Hardware\n\nFor running this stack effectively, I'd recommend:\n- At least 4-8 CPU cores\n- 16-32GB RAM (depending on your models)\n- SSD storage (faster vector searches)\n- If possible, a GPU with at least 8GB VRAM for better LLM performance\n\nLet's proceed with creating the Docker configuration:\n</think>\n\n# Docker Deployment for Code Assistant\n\nYou're right to be concerned about VPS resources. This app has several resource-intensive components, particularly LM Studio running a local LLM. Let me outline a Docker deployment strategy with some recommendations.\n\n## Resource Considerations\n\nYour application uses:\n1. **LM Studio's LLM** - The heaviest component, requiring significant CPU/GPU resources\n2. **Embedding model (BAAI/bge-m3)** - Moderate resource usage\n3. **Qdrant vector store** - Memory usage scales with indexed data size\n4. **LangChain processing** - Generally lightweight compared to models\n\n## Docker Setup\n\nHere's a complete solution:\n\n### 1. First, create a Dockerfile\n\n```dockerfile\nFROM python:3.10-slim\n\nWORKDIR /app\n\n# Install git and required dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Create directory for Qdrant storage\nRUN mkdir -p /tmp/langchain_qdrant\n\n# Expose the Gradio port\nEXPOSE 7860\n\n# Start the application\nCMD [\"python\", \"app.py\"]\n```\n\n### 2. Create requirements.txt\n\n```\nlangchain\nlangchain_huggingface\nlangchain_community\nlangchain_text_splitters\nlangchain_core\nlangchain_qdrant\nqdrant_client\ngradio\nrequests\nsentencetransformers\ntransformers\ntorch\n```\n\n### 3. Create docker-compose.yml\n\n```yaml\nversion: '3'\n\nservices:\n  code-assistant:\n    build: .\n    ports:\n      - \"7860:7860\"\n    volumes:\n      - ./data:/tmp/langchain_qdrant\n      - ./my_notes.txt:/app/my_notes.txt\n    environment:\n      - COLLECTION_NAME=code-assistant-store\n    restart: unless-stopped\n    deploy:\n      resources:\n        limits:\n          memory: 8G\n        reservations:\n          memory: 4G\n\n  lm-studio:\n    image: lmstudio/lmstudio:latest  # This is a placeholder - LM Studio might not have an official Docker image\n    ports:\n      - \"1234:1234\"\n    volumes:\n      - ./models:/models\n    environment:\n      - MODEL_PATH=/models/your-model-name\n    restart: unless-stopped\n    deploy:\n      resources:\n        limits:\n          memory: 8G\n        reservations:\n          memory: 4G\n```\n\n## Performance Optimization Strategies\n\nSince VPS resources are limited:\n\n1. **Use quantized models**: For LM Studio, prefer 4-bit or 8-bit quantized models (GGUF format)\n\n2. **Modify your code to use smaller embeddings**: \n   ```python\n   # Change this line\n   embeddings = HuggingFaceEmbeddings(\n       model_name=\"BAAI/bge-small-en-v1.5\", # A smaller embedding model\n       encode_kwargs={\"truncation\": True, \"max_length\": 512}\n   )\n   ```\n\n3. **Reduce context window size** to save memory:\n   ```python\n   llm.max_tokens = 512  # Reduced from 1024\n   ```\n\n4. **Consider running LM Studio separately**: Instead of containerizing both, run your application in Docker but LM Studio directly on the host with GPU access\n\n## Hosting Recommendations\n\n1. **Minimum VPS specs**:\n   - 4+ CPU cores\n   - 16GB+ RAM\n   - 50GB+ SSD\n   - GPU preferred but not mandatory (depends on model size)\n\n2. **Cloud providers with good price/performance**:\n   - Oracle Cloud (has free tier with 4 OCPUs/24GB RAM)\n   - Hetzner (affordable dedicated servers)\n   - Vast.ai (for GPU options)\n\n3. **Distributed setup**: Consider running the embedding and vector store on one server, and LM Studio on another more powerful machine\n\nLet me know if you need more specific advice for your deployment!\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":147}}},{"id":{"replica_id":0,"value":7},"start":13920,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":148}}},{"id":{"replica_id":0,"value":8},"start":13921,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":146}}},{"id":{"replica_id":0,"value":134},"start":14163,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":144}}},{"id":{"replica_id":0,"value":135},"start":19187,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":145}}}],"summary":"Docker Code Assistant Deployment","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}],"thought_process_output_sections":[{"range":{"start":14163,"end":15668}}]}