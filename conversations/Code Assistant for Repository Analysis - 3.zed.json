{"id":"b3df97c9-78ce-4f48-8a4f-128f0e2b2bb7","zed":"context","version":"0.4.0","text":"\n\n\nОбъясни подробно, как работает этот код и что он использует\nfrom pathlib import Path  # Добавляем для работы с путями к файлам\nimport os\nimport requests\nimport gradio as gr\nimport logging\nfrom typing import Optional, List\nfrom uuid import uuid4\nimport subprocess\nimport tempfile\n\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_text_splitters import SentenceTransformersTokenTextSplitter\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.documents import Document\nfrom langchain_qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import Distance, VectorParams\n\n# Настройка логирования\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nCOLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"code-assistant-store\")\n\n\nclass LMStudioLLM(LLM):\n    model: str = \"local-model\"\n    base_url: str = \"http://127.0.0.1:1234/v1\"\n    temperature: float = 0.7\n    max_tokens: int = 1024\n    top_p: float = 0.9\n    frequency_penalty: float = 0.0\n    presence_penalty: float = 0.0\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        payload = {\n            \"model\": self.model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            \"top_p\": self.top_p,\n            \"frequency_penalty\": self.frequency_penalty,\n            \"presence_penalty\": self.presence_penalty,\n        }\n        response = requests.post(f\"{self.base_url}/chat/completions\", json=payload)\n        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n\n    @property\n    def _llm_type(self) -> str:\n        return \"lmstudio-llm\"\n\n\ndef configure_qdrant() -> QdrantVectorStore:\n    \"\"\"Configure and return a Qdrant vector store.\"\"\"\n    logger.info(\"Configuring Qdrant client and vector store...\")\n\n    # Инициализация модели эмбедингов\n    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\", encode_kwargs={\"truncation\": True, \"max_length\": 512})\n    embeddings.embed_query(\"test\")  # Прогрев модели\n\n    # Создание клиента Qdrant\n    client = QdrantClient(path=\"/tmp/langchain_qdrant\")\n\n    # Создание коллекции, если она не существует\n    try:\n        client.create_collection(\n            collection_name=COLLECTION_NAME,\n            vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n        )\n    except ValueError:\n        logger.info(\"Collection already exists. Skipping creation.\")\n\n    # Создание векторного хранилища\n    vector_store = QdrantVectorStore(\n        client=client,\n        collection_name=COLLECTION_NAME,\n        embedding=embeddings,\n    )\n\n    logger.info(\"Qdrant configuration completed.\")\n    return vector_store\n\n\ndef clone_github_repo(repo_url: str, temp_dir: str) -> str:\n    \"\"\"Clone GitHub repository to temporary directory.\"\"\"\n    try:\n        repo_name = repo_url.split(\"/\")[-1]\n        if repo_name.endswith(\".git\"):\n            repo_name = repo_name[:-4]\n        repo_path = os.path.join(temp_dir, repo_name)\n\n        subprocess.run([\"git\", \"clone\", repo_url, repo_path], check=True)\n        logger.info(f\"Repository cloned successfully to {repo_path}\")\n        return repo_path\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Failed to clone repository: {e}\")\n        raise\n\n\ndef load_documents_from_repo(repo_path: str) -> List[Document]:\n    \"\"\"Load documents from repository.\"\"\"\n    documents = []\n\n    for root, _, files in os.walk(repo_path):\n        for file in files:\n            if file.endswith((\".txt\", \".md\", \".py\", \".js\", \".java\", \".cpp\", \".h\", \".html\", \".css\")):\n                file_path = os.path.join(root, file)\n                try:\n                    loader = TextLoader(file_path, encoding=\"utf-8\")\n                    loaded_docs = loader.load()\n                    for doc in loaded_docs:\n                        doc.metadata[\"source_file\"] = file_path  # Добавляем путь к файлу в метаданные\n                    documents.extend(loaded_docs)\n                    logger.info(f\"Loaded {file_path}\")\n                except Exception as e:\n                    logger.warning(f\"Could not load {file_path}: {e}\")\n\n    return documents\n\n\ndef add_repository_to_store(\n    vector_store: QdrantVectorStore, repo_url: str, chunk_size: int = 512, chunk_overlap: int = 50\n) -> None:\n    \"\"\"Add repository documents to vector store.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        try:\n            # Клонируем репозиторий\n            repo_path = clone_github_repo(repo_url, temp_dir)\n\n            # Загружаем документы\n            documents = load_documents_from_repo(repo_path)\n\n            if not documents:\n                raise ValueError(\"No documents found in repository\")\n\n            # Разделяем документы на чанки\n            splitter = SentenceTransformersTokenTextSplitter(\n                tokens_per_chunk=chunk_size, chunk_overlap=chunk_overlap, model_name=\"BAAI/bge-m3\"\n            )\n            texts = splitter.split_documents(documents)\n\n            # Добавляем в векторное хранилище\n            ids = [str(uuid4()) for _ in texts]\n            vector_store.add_documents(documents=texts, ids=ids)\n            logger.info(f\"Added {len(texts)} documents from repository to vector store\")\n\n        except Exception as e:\n            logger.error(f\"Error processing repository: {e}\")\n            raise\n\n\ndef load_or_create_vector_store(\n    vector_store: QdrantVectorStore, file_path: str = None, chunk_size: int = 512, chunk_overlap: int = 50\n) -> QdrantVectorStore:\n    \"\"\"Load or create vector store with documents.\"\"\"\n    # Проверяем, есть ли уже документы в хранилище\n    client = vector_store.client\n    collection_info = client.get_collection(COLLECTION_NAME)\n\n    if collection_info.vectors_count == 0 and file_path:\n        logger.info(\"No documents in vector store. Creating new index...\")\n\n        # Загрузка документов\n        loader = TextLoader(file_path)\n        documents = loader.load()\n        for doc in documents:\n            doc.metadata[\"source_file\"] = file_path  # Добавляем путь к файлу в метаданные\n\n        # Разделение документов на чанки\n        splitter = SentenceTransformersTokenTextSplitter(\n            tokens_per_chunk=chunk_size, chunk_overlap=chunk_overlap, model_name=\"BAAI/bge-m3\"\n        )\n        texts = splitter.split_documents(documents)\n\n        # Добавление документов в хранилище\n        ids = [str(uuid4()) for _ in texts]\n        vector_store.add_documents(documents=texts, ids=ids)\n        logger.info(f\"Added {len(texts)} documents to vector store.\")\n    else:\n        logger.info(f\"Vector store already contains {collection_info.vectors_count} vectors. Using existing index.\")\n\n    return vector_store\n\n\n# === Инициализация компонентов ===\nllm = LMStudioLLM()\nvector_store = configure_qdrant()\n\n# Загрузка или создание векторного хранилища\nFILE_PATH = r\"./my_notes.txt\"\nif os.path.exists(FILE_PATH):\n    vector_store = load_or_create_vector_store(vector_store, FILE_PATH)\nelse:\n    vector_store = load_or_create_vector_store(vector_store)\n\n\n# === Функции для Gradio ===\ndef answer_question(question, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda):\n    # Обновляем параметры LLM\n    llm.temperature = temperature\n    llm.max_tokens = max_tokens\n    llm.top_p = top_p\n\n    # Настраиваем ретривер с новыми параметрами\n    retriever = vector_store.as_retriever(\n        search_type=\"mmr\", search_kwargs={\"k\": k_results, \"fetch_k\": fetch_k, \"lambda_mult\": mmr_lambda}\n    )\n\n    # Сборка цепочки с текущими параметрами\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        retriever=retriever,\n        chain_type=\"stuff\",\n        chain_type_kwargs={\"prompt\": prompt_template},\n        return_source_documents=True,\n    )\n\n    response = qa_chain({\"query\": question})\n\n    # Получаем ответ и исходные документы\n    answer = response[\"result\"]\n    source_docs = response[\"source_documents\"]\n\n    # Формируем список уникальных исходных файлов\n    source_files = set()\n    for doc in source_docs:\n        if \"source_file\" in doc.metadata:\n            source_files.add(doc.metadata[\"source_file\"])\n\n    # Формируем итоговый ответ с указанием источников\n    if source_files:\n        answer += \"\\n\\nИсточники:\\n- \" + \"\\n- \".join(source_files)\n\n    return answer\n\n\ndef add_repository(repo_url, chunk_size, chunk_overlap):\n    try:\n        add_repository_to_store(vector_store, repo_url, chunk_size, chunk_overlap)\n        return \"Репозиторий успешно добавлен!\"\n    except Exception as e:\n        return f\"Ошибка при добавлении репозитория: {str(e)}\"\n\n\n# === Кастомный промпт ===\nprompt_template = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=(\n        \"Твоя задача - анализировать загруженный проект и отвечать на вопросы пользователя касательно проекта\\n\\n\"\n        \"Контекст:\\n{context}\\n\\nВопрос: {question}\\nОтвет:\"\n    ),\n)\n\n# === Интерфейс Gradio ===\ncss_file = Path(\"./gradio_style.css\")\n\nwith gr.Blocks(css=css_file.read_text()) as demo:\n    with gr.Tabs():\n        with gr.Tab(\"Задать вопрос\", id=\"question_tab\"):\n            with gr.Row():\n                with gr.Column():\n                    question_input = gr.Textbox(lines=2, placeholder=\"Задай вопрос по проекту...\", label=\"Вопрос\")\n                    with gr.Accordion(\"Настройки модели\", open=False):\n                        temperature = gr.Slider(0.1, 2.0, value=0.7, step=0.1, label=\"Temperature\")\n                        max_tokens = gr.Slider(128, 4096, value=1024, step=128, label=\"Max Tokens\")\n                        top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.1, label=\"Top-p\")\n                    with gr.Accordion(\"Настройки поиска\", open=False):\n                        k_results = gr.Slider(1, 10, value=5, step=1, label=\"Количество результатов\")\n                        fetch_k = gr.Slider(5, 50, value=20, step=5, label=\"Fetch K (кол-во кандидатов)\")\n                        mmr_lambda = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label=\"MMR Lambda (разнообразие)\")\n                    with gr.Row():\n                        ask_button = gr.Button(\"Спросить\", variant=\"primary\")\n                        clear_btn = gr.Button(\"Очистить\", variant=\"secondary\")\n                with gr.Column():\n                    answer_output = gr.Textbox(label=\"Ответ\", interactive=False)\n\n        with gr.Tab(\"Добавить репозиторий\", id=\"repo_tab\"):\n            with gr.Row():\n                with gr.Column():\n                    repo_url_input = gr.Textbox(\n                        placeholder=\"https://github.com/username/repository.git\", label=\"URL GitHub репозитория\"\n                    )\n                    with gr.Accordion(\"Настройки чанкинга\", open=False):\n                        chunk_size = gr.Slider(128, 1024, value=512, step=64, label=\"Размер чанка (токены)\")\n                        chunk_overlap = gr.Slider(0, 256, value=50, step=16, label=\"Перекрытие чанков (токены)\")\n                    with gr.Row():\n                        add_repo_button = gr.Button(\"Добавить репозиторий\", variant=\"primary\")\n                        clear_repo_btn = gr.Button(\"Очистить\", variant=\"secondary\")\n                with gr.Column():\n                    repo_status_output = gr.Textbox(label=\"Статус\", interactive=False)\n\n        with gr.Tab(\"Информация\", id=\"info_tab\"):\n            gr.Markdown(\"### Инструкция по использованию\")\n            gr.Markdown(\n                \"\"\"\n            1. **Задать вопрос** - задавайте вопросы о вашем проекте\n            2. **Добавить репозиторий** - загрузите новый репозиторий для анализа\n            3. Настройте параметры модели и поиска для лучших результатов\n            \"\"\"\n            )\n            gr.Markdown(\"### Технологии\")\n            gr.Markdown(\n                \"\"\"\n            - **LM Studio** - локальный LLM\n            - **LangChain** - обработка и анализ текста\n            - **Qdrant** - векторное хранилище\n            - **bge-m3** - модели эмбедингов\n            \"\"\"\n            )\n\n    # Обработчики кнопок очистки\n    clear_btn.click(\n        lambda: [None, 0.7, 1024, 0.9, 5, 20, 0.5],\n        outputs=[question_input, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda],\n    )\n    clear_repo_btn.click(lambda: [None, 512, 50], outputs=[repo_url_input, chunk_size, chunk_overlap])\n\n    ask_button.click(\n        answer_question,\n        inputs=[question_input, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda],\n        outputs=answer_output,\n    )\n    add_repo_button.click(\n        add_repository, inputs=[repo_url_input, chunk_size, chunk_overlap], outputs=repo_status_output\n    )\n\nif __name__ == \"__main__\":\n    demo.launch(server_name=\"0.0.0.0\", server_port=7860)\n    \n    \n<think>\nЭтот код представляет собой Python-приложение для локального помощника по коду с использованием локальной языковой модели, векторной базы данных и Gradio для веб-интерфейса. Давайте разберем его детально по частям:\n\n### Импорт библиотек\n\n```python\nfrom pathlib import Path\nimport os\nimport requests\nimport gradio as gr\nimport logging\nfrom typing import Optional, List\nfrom uuid import uuid4\nimport subprocess\nimport tempfile\n\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_text_splitters import SentenceTransformersTokenTextSplitter\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.documents import Document\nfrom langchain_qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import Distance, VectorParams\n```\n\nЗдесь импортируются необходимые библиотеки:\n- Стандартные библиотеки Python для работы с файлами, HTTP-запросами, логированием, типизацией\n- Gradio для создания веб-интерфейса\n- LangChain - фреймворк для работы с LLM (Language Model) моделями\n- Qdrant - векторная база данных для поиска похожих документов\n\n### Настройка логирования\n\n```python\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nCOLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"code-assistant-store\")\n```\n\nБазовая настройка логирования для записи информационных сообщений и получение названия коллекции для Qdrant из переменных окружения (с возможностью установки значения по умолчанию).\n\n### Класс для взаимодействия с LM Studio\n\n```python\nclass LMStudioLLM(LLM):\n    model: str = \"local-model\"\n    base_url: str = \"http://127.0.0.1:1234/v1\"\n    temperature: float = 0.7\n    max_tokens: int = 1024\n    top_p: float = 0.9\n    frequency_penalty: float = 0.0\n    presence_penalty: float = 0.0\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        payload = {\n            \"model\": self.model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            \"top_p\": self.top_p,\n            \"frequency_penalty\": self.frequency_penalty,\n            \"presence_penalty\": self.presence_penalty,\n        }\n        response = requests.post(f\"{self.base_url}/chat/completions\", json=payload)\n        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n\n    @property\n    def _llm_type(self) -> str:\n        return \"lmstudio-llm\"\n```\n\nСоздается класс LMStudioLLM, который наследуется от LLM в LangChain и предоставляет интерфейс для взаимодействия с локальной моделью, запущенной через LM Studio по адресу http://127.0.0.1:1234/v1. Класс отправляет HTTP-запросы к API LM Studio с параметрами генерации и получает ответы.\n\n### Функция настройки Qdrant\n\n```python\ndef configure_qdrant() -> QdrantVectorStore:\n    \"\"\"Configure and return a Qdrant vector store.\"\"\"\n    logger.info(\"Configuring Qdrant client and vector store...\")\n\n    # Инициализация модели эмбедингов\n    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\", encode_kwargs={\"truncation\": True, \"max_length\": 512})\n    embeddings.embed_query(\"test\")  # Прогрев модели\n\n    # Создание клиента Qdrant\n    client = QdrantClient(path=\"/tmp/langchain_qdrant\")\n\n    # Создание коллекции, если она не существует\n    try:\n        client.create_collection(\n            collection_name=COLLECTION_NAME,\n            vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n        )\n    except ValueError:\n        logger.info(\"Collection already exists. Skipping creation.\")\n\n    # Создание векторного хранилища\n    vector_store = QdrantVectorStore(\n        client=client,\n        collection_name=COLLECTION_NAME,\n        embedding=embeddings,\n    )\n\n    logger.info(\"Qdrant configuration completed.\")\n    return vector_store\n```\n\nФункция выполняет настройку векторного хранилища Qdrant:\n1. Создает модель эмбедингов (HuggingFaceEmbeddings), используя BAAI/bge-m3 для превращения текста в векторы\n2. Создает клиента Qdrant, указывая путь для локального хранения данных\n3. Создает коллекцию для хранения векторов (если она не существует)\n4. Инициализирует хранилище QdrantVectorStore\n\n### Функции для работы с GitHub репозиториями\n\n```python\ndef clone_github_repo(repo_url: str, temp_dir: str) -> str:\n    \"\"\"Clone GitHub repository to temporary directory.\"\"\"\n    try:\n        repo_name = repo_url.split(\"/\")[-1]\n        if repo_name.endswith(\".git\"):\n            repo_name = repo_name[:-4]\n        repo_path = os.path.join(temp_dir, repo_name)\n\n        subprocess.run([\"git\", \"clone\", repo_url, repo_path], check=True)\n        logger.info(f\"Repository cloned successfully to {repo_path}\")\n        return repo_path\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Failed to clone repository: {e}\")\n        raise\n\n\ndef load_documents_from_repo(repo_path: str) -> List[Document]:\n    \"\"\"Load documents from repository.\"\"\"\n    documents = []\n\n    for root, _, files in os.walk(repo_path):\n        for file in files:\n            if file.endswith((\".txt\", \".md\", \".py\", \".js\", \".java\", \".cpp\", \".h\", \".html\", \".css\")):\n                file_path = os.path.join(root, file)\n                try:\n                    loader = TextLoader(file_path, encoding=\"utf-8\")\n                    loaded_docs = loader.load()\n                    for doc in loaded_docs:\n                        doc.metadata[\"source_file\"] = file_path  # Добавляем путь к файлу в метаданные\n                    documents.extend(loaded_docs)\n                    logger.info(f\"Loaded {file_path}\")\n                except Exception as e:\n                    logger.warning(f\"Could not load {file_path}: {e}\")\n\n    return documents\n\n\ndef add_repository_to_store(\n    vector_store: QdrantVectorStore, repo_url: str, chunk_size: int = 512, chunk_overlap: int = 50\n) -> None:\n    \"\"\"Add repository documents to vector store.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        try:\n            # Клонируем репозиторий\n            repo_path = clone_github_repo(repo_url, temp_dir)\n\n            # Загружаем документы\n            documents = load_documents_from_repo(repo_path)\n\n            if not documents:\n                raise ValueError(\"No documents found in repository\")\n\n            # Разделяем документы на чанки\n            splitter = SentenceTransformersTokenTextSplitter(\n                tokens_per_chunk=chunk_size, chunk_overlap=chunk_overlap, model_name=\"BAAI/bge-m3\"\n            )\n            texts = splitter.split_documents(documents)\n\n            # Добавляем в векторное хранилище\n            ids = [str(uuid4()) for _ in texts]\n            vector_store.add_documents(documents=texts, ids=ids)\n            logger.info(f\"Added {len(texts)} documents from repository to vector store\")\n\n        except Exception as e:\n            logger.error(f\"Error processing repository: {e}\")\n            raise\n```\n\nЭти функции отвечают за работу с GitHub репозиториями:\n1. `clone_github_repo` - клонирует указанный репозиторий во временную директорию\n2. `load_documents_from_repo` - загружает документы из репозитория, выбирая файлы с определенными расширениями\n3. `add_repository_to_store` - добавляет документы из репозитория в векторное хранилище:\n   - Клонирует репозиторий\n   - Загружает документы\n   - Разбивает документы на чанки с помощью SentenceTransformersTokenTextSplitter\n   - Добавляет чанки в векторное хранилище с уникальными ID\n\n### Функция для загрузки или создания векторного хранилища\n\n```python\ndef load_or_create_vector_store(\n    vector_store: QdrantVectorStore, file_path: str = None, chunk_size: int = 512, chunk_overlap: int = 50\n) -> QdrantVectorStore:\n    \"\"\"Load or create vector store with documents.\"\"\"\n    # Проверяем, есть ли уже документы в хранилище\n    client = vector_store.client\n    collection_info = client.get_collection(COLLECTION_NAME)\n\n    if collection_info.vectors_count == 0 and file_path:\n        logger.info(\"No documents in vector store. Creating new index...\")\n\n        # Загрузка документов\n        loader = TextLoader(file_path)\n        documents = loader.load()\n        for doc in documents:\n            doc.metadata[\"source_file\"] = file_path  # Добавляем путь к файлу в метаданные\n\n        # Разделение документов на чанки\n        splitter = SentenceTransformersTokenTextSplitter(\n            tokens_per_chunk=chunk_size, chunk_overlap=chunk_overlap, model_name=\"BAAI/bge-m3\"\n        )\n        texts = splitter.split_documents(documents)\n\n        # Добавление документов в хранилище\n        ids = [str(uuid4()) for _ in texts]\n        vector_store.add_documents(documents=texts, ids=ids)\n        logger.info(f\"Added {len(texts)} documents to vector store.\")\n    else:\n        logger.info(f\"Vector store already contains {collection_info.vectors_count} vectors. Using existing index.\")\n\n    return vector_store\n```\n\nФункция проверяет, содержит ли хранилище векторы. Если нет, и указан путь к файлу, загружает документы из этого файла, разбивает их на чанки и добавляет в хранилище.\n\n### Инициализация компонентов\n\n```python\nllm = LMStudioLLM()\nvector_store = configure_qdrant()\n\n# Загрузка или создание векторного хранилища\nFILE_PATH = r\"./my_notes.txt\"\nif os.path.exists(FILE_PATH):\n    vector_store = load_or_create_vector_store(vector_store, FILE_PATH)\nelse:\n    vector_store = load_or_create_vector_store(vector_store)\n```\n\nЗдесь создаются основные компоненты приложения:\n1. Экземпляр LMStudioLLM для взаимодействия с языковой моделью\n2. Векторное хранилище с помощью функции configure_qdrant()\n3. Проверяется наличие файла my_notes.txt и, если он существует, загружается в хранилище\n\n### Функции для Gradio\n\n```python\ndef answer_question(question, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda):\n    # Обновляем параметры LLM\n    llm.temperature = temperature\n    llm.max_tokens = max_tokens\n    llm.top_p = top_p\n\n    # Настраиваем ретривер с новыми параметрами\n    retriever = vector_store.as_retriever(\n        search_type=\"mmr\", search_kwargs={\"k\": k_results, \"fetch_k\": fetch_k, \"lambda_mult\": mmr_lambda}\n    )\n\n    # Сборка цепочки с текущими параметрами\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        retriever=retriever,\n        chain_type=\"stuff\",\n        chain_type_kwargs={\"prompt\": prompt_template},\n        return_source_documents=True,\n    )\n\n    response = qa_chain({\"query\": question})\n\n    # Получаем ответ и исходные документы\n    answer = response[\"result\"]\n    source_docs = response[\"source_documents\"]\n\n    # Формируем список уникальных исходных файлов\n    source_files = set()\n    for doc in source_docs:\n        if \"source_file\" in doc.metadata:\n            source_files.add(doc.metadata[\"source_file\"])\n\n    # Формируем итоговый ответ с указанием источников\n    if source_files:\n        answer += \"\\n\\nИсточники:\\n- \" + \"\\n- \".join(source_files)\n\n    return answer\n\n\ndef add_repository(repo_url, chunk_size, chunk_overlap):\n    try:\n        add_repository_to_store(vector_store, repo_url, chunk_size, chunk_overlap)\n        return \"Репозиторий успешно добавлен!\"\n    except Exception as e:\n        return f\"Ошибка при добавлении репозитория: {str(e)}\"\n```\n\nЭти функции используются в интерфейсе Gradio:\n1. `answer_question` - отвечает на вопрос пользователя:\n   - Обновляет параметры LLM в соответствии с выбранными пользователем значениями\n   - Настраивает ретривер (retriever) для поиска релевантных документов\n   - Создает цепочку RetrievalQA, которая ищет релевантные документы и генерирует ответ на основе их содержимого\n   - Формирует список источников и добавляет их в конец ответа\n2. `add_repository` - добавляет новый репозиторий в хранилище\n\n### Шаблон промпта\n\n```python\nprompt_template = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=(\n        \"Твоя задача - анализировать загруженный проект и отвечать на вопросы пользователя касательно проекта\\n\\n\"\n        \"Контекст:\\n{context}\\n\\nВопрос: {question}\\nОтвет:\"\n    ),\n)\n```\n\nСоздается шаблон промпта, который будет использоваться для формирования запроса к языковой модели. Шаблон содержит две переменные: контекст (найденные документы) и вопрос пользователя.\n\n### Интерфейс Gradio\n\n```python\ncss_file = Path(\"./gradio_style.css\")\n\nwith gr.Blocks(css=css_file.read_text()) as demo:\n    with gr.Tabs():\n        with gr.Tab(\"Задать вопрос\", id=\"question_tab\"):\n            # ...\n        with gr.Tab(\"Добавить репозиторий\", id=\"repo_tab\"):\n            # ...\n        with gr.Tab(\"Информация\", id=\"info_tab\"):\n            # ...\n\n    # Обработчики кнопок очистки\n    clear_btn.click(\n        lambda: [None, 0.7, 1024, 0.9, 5, 20, 0.5],\n        outputs=[question_input, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda],\n    )\n    clear_repo_btn.click(lambda: [None, 512, 50], outputs=[repo_url_input, chunk_size, chunk_overlap])\n\n    ask_button.click(\n        answer_question,\n        inputs=[question_input, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda],\n        outputs=answer_output,\n    )\n    add_repo_button.click(\n        add_repository, inputs=[repo_url_input, chunk_size, chunk_overlap], outputs=repo_status_output\n    )\n\nif __name__ == \"__main__\":\n    demo.launch(server_name=\"0.0.0.0\", server_port=7860)\n```\n\nСоздается интерфейс Gradio с тремя вкладками:\n1. \"Задать вопрос\" - позволяет пользователю задать вопрос о коде и получить ответ\n2. \"Добавить репозиторий\" - позволяет добавить новый GitHub репозиторий в хранилище\n3. \"Информация\" - содержит информацию о приложении и инструкции по использованию\n\nВ коде также определяются обработчики событий для взаимодействия пользователя с интерфейсом, и если скрипт запускается напрямую (не импортируется), запускается веб-сервер Gradio на порту 7860, доступный извне.\n</think>\n\n# Подробный анализ кода: Локальный ассистент по коду\n\nЭтот код представляет собой приложение для создания локального ассистента по коду, который может отвечать на вопросы о проектах, анализируя их исходный код. Давайте рассмотрим его структуру и компоненты.\n\n## Основные компоненты и технологии:\n\n1. **LangChain** - фреймворк для работы с языковыми моделями и создания приложений на их основе\n2. **Qdrant** - векторная база данных для семантического поиска\n3. **LM Studio** - средство для запуска локальных языковых моделей\n4. **Gradio** - библиотека для создания веб-интерфейсов\n5. **HuggingFace Embeddings** - для создания векторных представлений текста\n\n## Организация кода и функциональность:\n\n### 1. Создание кастомного LLM:\n```python\nclass LMStudioLLM(LLM):\n    # ...\n```\nЭтот класс создает интерфейс для взаимодействия с локальной языковой моделью через LM Studio по адресу `http://127.0.0.1:1234/v1`. Отправляет запросы через API, совместимый с OpenAI.\n\n### 2. Настройка векторного хранилища Qdrant:\n```python\ndef configure_qdrant() -> QdrantVectorStore:\n    # ...\n```\nФункция инициализирует:\n- Модель эмбедингов `BAAI/bge-m3` для преобразования текста в векторы\n- Локальное хранилище Qdrant для этих векторов\n- Создает коллекцию для хранения векторов размерностью 1024 с косинусной метрикой сходства\n\n### 3. Работа с репозиториями кода:\n```python\ndef clone_github_repo(repo_url: str, temp_dir: str) -> str:\n    # ...\n\ndef load_documents_from_repo(repo_path: str) -> List[Document]:\n    # ...\n\ndef add_repository_to_store(vector_store: QdrantVectorStore, repo_url: str, ...):\n    # ...\n```\nЭти функции обеспечивают:\n- Клонирование GitHub репозитория\n- Загрузку документов с поддерживаемыми расширениями (.py, .js, .java, .cpp и др.)\n- Разбиение документов на части (чанки) для векторизации\n- Сохранение их в векторной базе данных\n\n### 4. Загрузка и подготовка данных:\n```python\ndef load_or_create_vector_store(vector_store: QdrantVectorStore, file_path: str = None, ...):\n    # ...\n```\nФункция проверяет наличие данных в хранилище, при необходимости загружает документы из указанного файла и индексирует их.\n\n### 5. Функции обработки вопросов:\n```python\ndef answer_question(question, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda):\n    # ...\n```\nЗдесь используется цепочка `RetrievalQA`, которая:\n- Ищет в векторной базе релевантные к вопросу документы (через MMR - Maximum Marginal Relevance)\n- Объединяет найденные документы и вопрос пользователя в промпт\n- Отправляет это языковой модели\n- Возвращает ответ с указанием источников\n\n### 6. Веб-интерфейс на Gradio:\n\nИнтерфейс имеет три вкладки:\n- **Задать вопрос** - позволяет задавать вопросы с настройкой параметров модели и поиска\n- **Добавить репозиторий** - для загрузки новых GitHub репозиториев\n- **Информация** - инструкция и описание технологий\n\n### 7. Особенности реализации:\n\n- **Разбиение на чанки** использует токенайзер той же модели, что и для эмбедингов\n- **MMR поиск** позволяет получать разнообразные результаты, не только самые похожие\n- **Исходные файлы** добавляются в метаданные для отслеживания источников\n- **Настройки генерации** (temperature, max_tokens и т.д.) доступны через UI\n\n## Запуск приложения:\nПриложение запускается на всех интерфейсах (0.0.0.0) на порту 7860, что позволяет получить доступ к нему с других устройств в сети.\n\nЭто комплексное решение для анализа кода и ответов на вопросы, работающее полностью локально, без необходимости отправки данных внешним сервисам.\n\n\ngradio.launch(share=True) что делает?\n<think>\nДавайте разберем что делает метод `gradio.launch(share=True)` в контексте библиотеки Gradio:\n\nМетод `launch()` в Gradio используется для запуска веб-интерфейса. Когда вы вызываете этот метод, Gradio создает локальный веб-сервер, который обслуживает ваш интерфейс.\n\nПараметр `share=True` имеет особое значение:\n\n1. **Создание временного публичного URL**: Когда вы задаете `share=True`, Gradio не только запускает локальный сервер, но и создает временный общедоступный URL, который перенаправляет трафик на ваш локальный сервер. Это делается через сервис Gradio, который создает туннель до вашего локального компьютера.\n\n2. **Временный внешний доступ**: Этот временный URL позволяет людям в интернете получить доступ к вашему приложению Gradio, даже если оно запущено на вашем локальном компьютере. URL обычно имеет вид `https://xxxx.gradio.app` или `https://xxxx.gradio.live`.\n\n3. **Ограниченный срок действия**: Этот публичный URL временный и обычно действителен только в течение 72 часов или пока запущен ваш локальный сервер.\n\n4. **Без настройки портов**: Вам не нужно настраивать перенаправление портов на вашем маршрутизаторе или брандмауэре, что делает этот метод очень удобным для быстрого обмена демонстрациями.\n\nПример использования:\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\ndemo.launch(share=True)  # Создаст временную общедоступную ссылку\n```\n\nЭтот функционал особенно полезен в следующих сценариях:\n- Быстрая демонстрация вашего проекта другим людям\n- Тестирование приложения на других устройствах (например, на мобильном)\n- Совместная работа над проектом без необходимости деплоя на сервер\n- Презентации или обучающие сессии\n\nВажно отметить, что, хотя это удобно для разработки и тестирования, для продакшн-использования рекомендуется разворачивать Gradio на настоящем сервере, а не использовать эту функцию общего доступа.\n\nВ контексте кода, который вы показали ранее, файл не использует `share=True`, а вместо этого использует:\n```python\ndemo.launch(server_name=\"0.0.0.0\", server_port=7860)\n```\n\nЭто делает интерфейс доступным на всех сетевых интерфейсах (за счет использования \"0.0.0.0\") на порту 7860, но без создания публичного URL. То есть интерфейс будет доступен только в вашей локальной сети.\n</think>\n\n# Что делает `gradio.launch(share=True)`\n\nПараметр `share=True` в методе `gradio.launch()` позволяет создать временную общедоступную ссылку на ваше Gradio-приложение через интернет. Вот как это работает:\n\n## Основное назначение:\n\nКогда вы добавляете `share=True` к методу `launch()`, Gradio:\n\n1. **Создает временный публичный URL** - обычно в формате `https://xxxx.gradio.live` или `https://xxxx.gradio.app`\n\n2. **Устанавливает туннель** между этим публичным URL и вашим локальным веб-сервером Gradio\n\n3. **Выводит эту ссылку в консоль**, чтобы вы могли её скопировать и поделиться\n\n## Преимущества:\n\n- **Доступ из любой точки мира** - люди могут использовать ваше приложение без необходимости запускать код\n- **Не требует настройки портов** - не нужно конфигурировать маршрутизатор или брандмауэр\n- **Работает через NAT и файрволы** - даже если ваш компьютер находится за корпоративным файрволом\n\n## Ограничения:\n\n- Ссылка **временная** (обычно активна 72 часа или пока запущено приложение)\n- Имеет **ограничения по пропускной способности**\n- **Не рекомендуется для продакшн-приложений** из соображений безопасности и стабильности\n\n## Сравнение с кодом в вашем примере:\n\nВ вашем коде используется:\n```python\ndemo.launch(server_name=\"0.0.0.0\", server_port=7860)\n```\n\nЭто отличается от `share=True` тем, что:\n- `server_name=\"0.0.0.0\"` - делает приложение доступным на всех сетевых интерфейсах компьютера\n- `server_port=7860` - указывает конкретный порт\n- НЕ создаёт публичный URL - приложение доступно только в локальной сети\n\n## Пример использования share=True:\n\n```python\nimport gradio as gr\n\ndef hello(name):\n    return f\"Привет, {name}!\"\n\ndemo = gr.Interface(fn=hello, inputs=\"text\", outputs=\"text\")\ndemo.launch(share=True)  # Создаст и выведет публичную ссылку\n```\n\nПараметр `share=True` особенно полезен для быстрой демонстрации проектов, тестирования на мобильных устройствах или когда нужно предоставить временный доступ к приложению без разворачивания его на сервере.\nДля чего используются лонгчейны?\n\n\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":1315}}},{"id":{"replica_id":0,"value":27},"start":112,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":1316}}},{"id":{"replica_id":0,"value":37},"start":14408,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":1317}}},{"id":{"replica_id":0,"value":38},"start":37377,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":1318}}},{"id":{"replica_id":0,"value":107},"start":37378,"metadata":{"role":"assistant","status":{"Error":"cloud language model completion failed with status 400 Bad Request: messages: final assistant content cannot end with trailing whitespace"},"timestamp":{"replica_id":0,"value":1319}}},{"id":{"replica_id":0,"value":108},"start":37379,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":1320}}},{"id":{"replica_id":0,"value":176},"start":37426,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":1321}}},{"id":{"replica_id":0,"value":177},"start":44438,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":1322}}},{"id":{"replica_id":0,"value":302},"start":44500,"metadata":{"role":"assistant","status":{"Error":"cloud language model completion failed with status 451 Unavailable For Legal Reasons: access to Anthropic models is not available in your region (RU)"},"timestamp":{"replica_id":0,"value":1323}}},{"id":{"replica_id":0,"value":303},"start":44501,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":1324}}}],"summary":"Code Assistant for Repository Analysis","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}],"thought_process_output_sections":[{"range":{"start":14408,"end":31835}},{"range":{"start":37426,"end":41282}}]}