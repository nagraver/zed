{"id":"e0842adf-c0ed-4129-a5ac-efdcfe11ed8d","zed":"context","version":"0.4.0","text":"\n\nКак использовать эмбеддинги?\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nproject_path = \"./repo_clone\"\n\ndef analyze_single_file(file_path: str) -> str:\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            code_content = f.read()\n\n        response = client.chat.completions.create(\n            model=\"llama-3.1-8b-lexi-uncensored-v2\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You're a senior code analyst. Analyze the provided code and describe:\\n\"\n                        \"1. Основное назначение файла\\n\"\n                        \"2. Ключевые функции/классы\\n\"\n                        \"3. Входные/выходные данные (если применимо)\\n\"\n                        \"4. Зависимости (если видны)\\n\"\n                        \"5. Особенности реализации\\n\"\n                        \"Будь лаконичным (3-5 предложений).\\n\"\n                        \"Отвечай на русском языке.\"\n                    ),\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Проанализируй этот код:\\n\\n{code_content}\",\n                },\n            ],\n            temperature=0.2,\n        )\n\n        if response.choices and len(response.choices) > 0:\n            return response.choices[0].message.content.strip()\n        return \"Не удалось проанализировать код\"\n\n    except Exception as e:\n        return f\"Ошибка при анализе файла {file_path}: {str(e)}\"\n\ndef analyze_code_project(repo_path: str) -> str:\n    allowed_extensions = (\n        \".py\", \".md\", \".txt\", \".js\", \".java\", \n        \".cpp\", \".h\", \".html\", \".css\",\n    )\n    \n    analysis_results = []\n    total_files = 0\n    analyzed_files = 0\n    \n    for root, dirs, files in os.walk(repo_path):\n        if \".git\" in dirs:\n            dirs.remove(\".git\")\n\n        for file in files:\n            if not file.lower().endswith(allowed_extensions):\n                continue\n            total_files += 1\n            file_path = os.path.join(root, file)\n            \n            # Анализируем каждый файл отдельно\n            file_analysis = analyze_single_file(file_path)\n            analysis_results.append(f\"Файл: {file_path}\\nАнализ: {file_analysis}\\n\")\n            analyzed_files += 1\n    \n    if not analysis_results:\n        return \"В проекте не найдено файлов для анализа.\"\n    \n    # Создаем суммарный отчет\n    summary = (\n        f\"Общий анализ проекта: {repo_path}\\n\"\n        f\"Всего файлов: {total_files}\\n\"\n        f\"Проанализировано файлов: {analyzed_files}\\n\\n\"\n        \"Детальный анализ по файлам:\\n\\n\" + \n        \"\\n\".join(analysis_results)\n    )\n    \n    return summary\n\nif __name__ == \"__main__\":\n    analysis_result = analyze_code_project(project_path)\n    print(\"\\nРезультат анализа проекта:\")\n    print(analysis_result)\n<think>\nThis code appears to be a script for analyzing code repositories. Let's understand what it does and how it works, and then I'll address how to integrate embeddings into it.\n\nThe code consists of several functions:\n1. `analyze_single_file`: Takes a file path, reads the content, and sends it to an LLM (in this case, Llama 3.1) to analyze the code. The LLM provides a brief analysis in Russian covering the file's purpose, key functions/classes, inputs/outputs, dependencies, and implementation details.\n\n2. `analyze_code_project`: Walks through a repository, finds files with allowed extensions, and calls `analyze_single_file` for each file. It then aggregates the results into a summary report.\n\nThe main part of the code runs `analyze_code_project` on a path defined by `project_path` and prints the analysis.\n\nNow, regarding embeddings and how to integrate them into this code:\n\nEmbeddings are vector representations of text (or other data) that capture semantic meaning. They can be useful in this context for several purposes:\n\n1. **Similarity search**: You could generate embeddings for all code files and find similar files or code snippets.\n2. **Clustering**: Group similar files together to understand the structure of the codebase.\n3. **More efficient analysis**: Instead of analyzing each file individually, you could use embeddings to identify representative files or to skip nearly identical files.\n4. **Contextual analysis**: Use embeddings to understand the relationships between different files in the codebase.\n\nTo implement embeddings in this code, you would need to:\n\n1. Generate embeddings for each code file\n2. Store these embeddings (in memory or in a vector database)\n3. Use these embeddings for tasks like similarity search or clustering\n4. Potentially modify the analysis process to leverage these embeddings\n\nLet me outline a solution that integrates embeddings into this code:\n</think>\n\n# Применение эмбеддингов для улучшения анализа кода\n\nЭмбеддинги - это векторные представления текста, которые можно использовать для семантического анализа и сравнения. Вот как можно интегрировать их в ваш код:\n\n```python\nfrom openai import OpenAI\nimport os\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nproject_path = \"./repo_clone\"\n\n# Функция для создания эмбеддингов\ndef create_embedding(text: str) -> list:\n    response = client.embeddings.create(\n        model=\"all-minilm-l6-v2\", # или другая поддерживаемая модель\n        input=text\n    )\n    return response.data[0].embedding\n\n# Хранилище для эмбеддингов файлов\nfile_embeddings = {}\n\ndef analyze_single_file(file_path: str) -> str:\n    # Ваш текущий код...\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            code_content = f.read()\n            \n        # Создаем эмбеддинг для файла\n        embedding = create_embedding(code_content)\n        file_embeddings[file_path] = embedding\n        \n        # Остальной код анализа файла...\n        # ...\n    except Exception as e:\n        return f\"Ошибка при анализе файла {file_path}: {str(e)}\"\n\ndef find_similar_files(target_file: str, threshold: float = 0.8) -> list:\n    \"\"\"Находит файлы, похожие на целевой файл.\"\"\"\n    if target_file not in file_embeddings:\n        return []\n    \n    target_embedding = file_embeddings[target_file]\n    similar_files = []\n    \n    for file_path, embedding in file_embeddings.items():\n        if file_path == target_file:\n            continue\n        \n        similarity = cosine_similarity(\n            [target_embedding], \n            [embedding]\n        )[0][0]\n        \n        if similarity >= threshold:\n            similar_files.append((file_path, similarity))\n    \n    return sorted(similar_files, key=lambda x: x[1], reverse=True)\n\ndef analyze_code_project(repo_path: str) -> str:\n    # Ваш текущий код...\n    \n    # После анализа всех файлов можно добавить:\n    # Группируем похожие файлы\n    file_clusters = {}\n    processed_files = set()\n    \n    for file_path in file_embeddings:\n        if file_path in processed_files:\n            continue\n            \n        similar = find_similar_files(file_path)\n        if similar:\n            cluster = [file_path] + [f[0] for f in similar]\n            file_clusters[file_path] = cluster\n            processed_files.update(cluster)\n    \n    # Добавляем информацию о связанных файлах в отчет\n    summary += \"\\n\\nГруппы связанных файлов:\\n\"\n    for main_file, cluster in file_clusters.items():\n        summary += f\"\\nГруппа: {os.path.basename(main_file)}\\n\"\n        summary += \"\\n\".join(f\"- {os.path.basename(f)}\" for f in cluster)\n    \n    return summary\n```\n\n## Другие способы использования эмбеддингов:\n\n1. **Поиск по коду**: Создайте эмбеддинги для частей кода и реализуйте семантический поиск.\n2. **Классификация файлов**: Группируйте файлы по их назначению (конфигурация, тесты, бизнес-логика).\n3. **Обнаружение дубликатов**: Находите повторяющийся код в разных частях проекта.\n4. **Контекстный анализ**: Анализируйте файл с учетом связанных файлов.\n5. **Построение графа взаимосвязей**: Визуализируйте связи между компонентами кодовой базы.\n\nДля работы с большими проектами рекомендую использовать специальные векторные базы данных (Pinecone, Milvus, Qdrant) вместо хранения эмбеддингов в памяти.\n\nНапиши код полностью\nНапиши код полностью\nНапиши код полностью\n<think>\nI'll write the complete code that integrates embeddings for code analysis. The code should include:\n\n1. The original code for analyzing files\n2. Functions for creating and working with embeddings\n3. Similarity analysis between files\n4. Clustering of related files\n5. Enhanced reporting that includes these relationships\n\nHere's the complete code:\n</think>\n\n```python\nfrom openai import OpenAI\nimport os\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom typing import List, Tuple, Dict, Set\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nproject_path = \"./repo_clone\"\n\n# Хранилище для эмбеддингов файлов\nfile_embeddings = {}\nfile_analyses = {}\n\ndef create_embedding(text: str) -> List[float]:\n    \"\"\"Создаёт эмбеддинг для текста с помощью модели\"\"\"\n    try:\n        response = client.embeddings.create(\n            model=\"all-minilm-l6-v2\",  # Используйте модель, которая поддерживается вашим сервером\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Ошибка при создании эмбеддинга: {str(e)}\")\n        # Возвращаем пустой вектор в случае ошибки\n        return [0.0] * 384  # Размерность может отличаться в зависимости от модели\n\ndef truncate_text(text: str, max_tokens: int = 8000) -> str:\n    \"\"\"Обрезает текст до указанного количества токенов (примерно)\"\"\"\n    tokens = text.split()\n    if len(tokens) <= max_tokens:\n        return text\n    return \" \".join(tokens[:max_tokens])\n\ndef analyze_single_file(file_path: str) -> str:\n    \"\"\"Анализирует отдельный файл и создаёт его эмбеддинг\"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            code_content = f.read()\n        \n        # Создаем эмбеддинг для файла\n        truncated_content = truncate_text(code_content)\n        embedding = create_embedding(truncated_content)\n        file_embeddings[file_path] = embedding\n        \n        # Запрашиваем анализ от языковой модели\n        response = client.chat.completions.create(\n            model=\"llama-3.1-8b-lexi-uncensored-v2\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You're a senior code analyst. Analyze the provided code and describe:\\n\"\n                        \"1. Основное назначение файла\\n\"\n                        \"2. Ключевые функции/классы\\n\"\n                        \"3. Входные/выходные данные (если применимо)\\n\"\n                        \"4. Зависимости (если видны)\\n\"\n                        \"5. Особенности реализации\\n\"\n                        \"Будь лаконичным (3-5 предложений).\\n\"\n                        \"Отвечай на русском языке.\"\n                    ),\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Проанализируй этот код:\\n\\n{truncated_content}\",\n                },\n            ],\n            temperature=0.2,\n        )\n\n        if response.choices and len(response.choices) > 0:\n            analysis = response.choices[0].message.content.strip()\n            file_analyses[file_path] = analysis\n            return analysis\n        return \"Не удалось проанализировать код\"\n\n    except Exception as e:\n        return f\"Ошибка при анализе файла {file_path}: {str(e)}\"\n\ndef find_similar_files(target_file: str, threshold: float = 0.75) -> List[Tuple[str, float]]:\n    \"\"\"Находит файлы, похожие на целевой файл по косинусному сходству.\"\"\"\n    if target_file not in file_embeddings:\n        return []\n    \n    target_embedding = file_embeddings[target_file]\n    similar_files = []\n    \n    for file_path, embedding in file_embeddings.items():\n        if file_path == target_file:\n            continue\n        \n        similarity = cosine_similarity(\n            [target_embedding], \n            [embedding]\n        )[0][0]\n        \n        if similarity >= threshold:\n            similar_files.append((file_path, similarity))\n    \n    return sorted(similar_files, key=lambda x: x[1], reverse=True)\n\ndef cluster_files(similarity_threshold: float = 0.75) -> Dict[str, List[str]]:\n    \"\"\"Группирует файлы в кластеры на основе их сходства.\"\"\"\n    file_clusters = {}\n    processed_files = set()\n    \n    for file_path in file_embeddings.keys():\n        if file_path in processed_files:\n            continue\n            \n        similar = find_similar_files(file_path, threshold=similarity_threshold)\n        if similar:\n            cluster = [file_path] + [f[0] for f in similar]\n            file_clusters[file_path] = cluster\n            processed_files.update(cluster)\n        else:\n            file_clusters[file_path] = [file_path]\n            processed_files.add(file_path)\n    \n    return file_clusters\n\ndef generate_file_categories(files: List[str]) -> Dict[str, List[str]]:\n    \"\"\"Группирует файлы по их типу и функциональности.\"\"\"\n    categories = defaultdict(list)\n    \n    for file_path in files:\n        file_ext = os.path.splitext(file_path)[1].lower()\n        \n        # Группировка по расширениям\n        if file_ext in ['.py']:\n            categories['Python'].append(file_path)\n        elif file_ext in ['.js']:\n            categories['JavaScript'].append(file_path)\n        elif file_ext in ['.java']:\n            categories['Java'].append(file_path)\n        elif file_ext in ['.cpp', '.h', '.c']:\n            categories['C/C++'].append(file_path)\n        elif file_ext in ['.html', '.htm']:\n            categories['HTML'].append(file_path)\n        elif file_ext in ['.css']:\n            categories['CSS'].append(file_path)\n        elif file_ext in ['.md']:\n            categories['Documentation'].append(file_path)\n        else:\n            categories['Other'].append(file_path)\n            \n        # Дополнительная группировка по имени файла\n        if 'test' in file_path.lower() or 'spec' in file_path.lower():\n            categories['Tests'].append(file_path)\n        if 'config' in file_path.lower() or 'settings' in file_path.lower():\n            categories['Configuration'].append(file_path)\n        if 'utils' in file_path.lower() or 'helpers' in file_path.lower():\n            categories['Utilities'].append(file_path)\n            \n    return dict(categories)\n\ndef create_project_summary(file_analyses: Dict[str, str], file_clusters: Dict[str, List[str]], \n                          categories: Dict[str, List[str]]) -> str:\n    \"\"\"Создаёт общий отчет о проекте на основе анализа файлов и их группировки.\"\"\"\n    # Получаем самые центральные файлы (которые имеют много связей)\n    cluster_sizes = {main_file: len(cluster) for main_file, cluster in file_clusters.items()}\n    central_files = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)[:5]\n    \n    # Формируем суммарный отчет\n    summary = \"# Анализ проекта\\n\\n\"\n    \n    # Общая информация\n    summary += f\"## Общая информация\\n\\n\"\n    summary += f\"- Всего файлов проанализировано: {len(file_analyses)}\\n\"\n    summary += f\"- Количество кластеров похожих файлов: {len(file_clusters)}\\n\\n\"\n    \n    # Основные файлы проекта\n    summary += \"## Ключевые файлы проекта\\n\\n\"\n    for file_path, cluster_size in central_files:\n        rel_path = os.path.relpath(file_path, project_path)\n        file_analysis = file_analyses.get(file_path, \"Анализ отсутствует\")\n        summary += f\"### {rel_path} (связан с {cluster_size-1} файлами)\\n\\n\"\n        summary += f\"{file_analysis}\\n\\n\"\n    \n    # Распределение файлов по категориям\n    summary += \"## Распределение файлов по категориям\\n\\n\"\n    for category, files in categories.items():\n        summary += f\"### {category}: {len(files)} файлов\\n\\n\"\n        for file_path in files[:3]:  # Показываем только первые 3 файла в категории\n            rel_path = os.path.relpath(file_path, project_path)\n            summary += f\"- {rel_path}\\n\"\n        if len(files) > 3:\n            summary += f\"- ... и еще {len(files) - 3} файлов\\n\"\n        summary += \"\\n\"\n    \n    # Группы связанных файлов\n    summary += \"## Группы связанных файлов\\n\\n\"\n    top_clusters = sorted(file_clusters.items(), key=lambda x: len(x[1]), reverse=True)[:10]\n    for main_file, cluster in top_clusters:\n        if len(cluster) > 1:  # Показываем только кластеры с более чем одним файлом\n            rel_path = os.path.relpath(main_file, project_path)\n            summary += f\"### Группа: {rel_path}\\n\\n\"\n            for file in cluster:\n                rel_file = os.path.relpath(file, project_path)\n                if file == main_file:\n                    summary += f\"- **{rel_file}** (основной файл)\\n\"\n                else:\n                    similar_files = find_similar_files(main_file)\n                    similarity = next((sim for f, sim in similar_files if f == file), 0)\n                    summary += f\"- {rel_file} (сходство: {similarity:.2f})\\n\"\n            summary += \"\\n\"\n    \n    return summary\n\ndef analyze_code_project(repo_path: str) -> str:\n    \"\"\"Анализирует весь проект, создавая эмбеддинги и анализ для каждого файла.\"\"\"\n    allowed_extensions = (\n        \".py\", \".md\", \".txt\", \".js\", \".java\", \n        \".cpp\", \".h\", \".html\", \".css\",\n    )\n    \n    analysis_results = []\n    total_files = 0\n    analyzed_files = 0\n    \n    # Очищаем хранилища от предыдущих анализов\n    file_embeddings.clear()\n    file_analyses.clear()\n    \n    print(\"Анализируем файлы проекта...\")\n    for root, dirs, files in os.walk(repo_path):\n        if \".git\" in dirs:\n            dirs.remove(\".git\")\n        if \"venv\" in dirs:\n            dirs.remove(\"venv\")\n        if \"node_modules\" in dirs:\n            dirs.remove(\"node_modules\")\n\n        for file in files:\n            if not file.lower().endswith(allowed_extensions):\n                continue\n            total_files += 1\n            file_path = os.path.join(root, file)\n            \n            print(f\"Анализ файла: {file_path}\")\n            # Анализируем каждый файл отдельно\n            file_analysis = analyze_single_file(file_path)\n            analysis_results.append(f\"Файл: {file_path}\\nАнализ: {file_analysis}\\n\")\n            analyzed_files += 1\n    \n    if not analysis_results:\n        return \"В проекте не найдено файлов для анализа.\"\n    \n    print(\"Группируем похожие файлы...\")\n    file_clusters = cluster_files()\n    \n    print(\"Категоризируем файлы...\")\n    file_categories = generate_file_categories(list(file_analyses.keys()))\n    \n    print(\"Создаём итоговый отчет...\")\n    project_summary = create_project_summary(file_analyses, file_clusters, file_categories)\n    \n    # Создаем детальный отчет по файлам\n    detailed_analysis = (\n        f\"# Детальный анализ проекта: {repo_path}\\n\\n\"\n        f\"Всего файлов: {total_files}\\n\"\n        f\"Проанализировано файлов: {analyzed_files}\\n\\n\"\n        \"## Детальный анализ по файлам:\\n\\n\" + \n        \"\\n\".join(analysis_results)\n    )\n    \n    # Сохраняем отчеты в файлы\n    with open(\"project_summary.md\", \"w\", encoding=\"utf-8\") as f:\n        f.write(project_summary)\n    \n    with open(\"detailed_analysis.md\", \"w\", encoding=\"utf-8\") as f:\n        f.write(detailed_analysis)\n    \n    return project_summary\n\ndef visualize_file_relationships(file_clusters: Dict[str, List[str]], output_file: str = \"code_relationships.png\"):\n    \"\"\"Визуализирует связи между файлами в проекте.\"\"\"\n    import networkx as nx\n    \n    G = nx.Graph()\n    \n    # Добавляем узлы и ребра\n    for main_file, cluster in file_clusters.items():\n        main_name = os.path.basename(main_file)\n        G.add_node(main_name)\n        \n        for file in cluster:\n            if file != main_file:\n                file_name = os.path.basename(file)\n                G.add_node(file_name)\n                G.add_edge(main_name, file_name)\n    \n    # Рисуем граф\n    plt.figure(figsize=(12, 10))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, node_color=\"skyblue\", node_size=1500, edge_color=\"gray\")\n    plt.title(\"Связи между файлами в проекте\")\n    plt.savefig(output_file)\n    \n    return output_file\n\nif __name__ == \"__main__\":\n    print(f\"Начинаем анализ проекта: {project_path}\")\n    analysis_result = analyze_code_project(project_path)\n    \n    print(\"\\nРезультат анализа проекта:\")\n    print(f\"Краткий анализ сохранен в файл project_summary.md\")\n    print(f\"Подробный анализ сохранен в файл detailed_analysis.md\")\n    \n    # Визуализируем связи, если установлен networkx\n    try:\n        file_clusters = cluster_files()\n        viz_file = visualize_file_relationships(file_clusters)\n        print(f\"Визуализация связей между файлами сохранена в {viz_file}\")\n    except ImportError:\n        print(\"Для визуализации связей установите библиотеку networkx и matplotlib\")\n```\n\nЭтот код:\n\n1. Анализирует каждый файл в проекте и создает его векторное представление (эмбеддинг)\n2. Находит похожие файлы на основе косинусного сходства эмбеддингов\n3. Группирует файлы в кластеры на основе их сходства\n4. Категоризирует файлы по типам (Python, JavaScript, тесты и т.д.)\n5. Создает два отчета:\n   - Краткий аналитический отчет с выделением ключевых файлов и групп\n   - Детальный отчет со всеми проанализированными файлами\n6. Визуализирует связи между файлами в виде графа (если установлен networkx)\n\nДля использования просто укажите путь к вашему проекту в переменной `project_path` и запустите скрипт.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":606}}},{"id":{"replica_id":0,"value":20},"start":56,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":607}}},{"id":{"replica_id":0,"value":25},"start":3346,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":614}}},{"id":{"replica_id":0,"value":167},"start":9741,"metadata":{"role":"assistant","status":{"Error":"cloud language model completion failed with status 400 Bad Request: messages.1.content.0.type: Expected `thinking` or `redacted_thinking`, but found `text`. When `thinking` is enabled, a final `assistant` message must start with a thinking block. We recommend you include thinking blocks from previous turns. To avoid this requirement, disable `thinking`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking"},"timestamp":{"replica_id":0,"value":615}}},{"id":{"replica_id":0,"value":168},"start":9780,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":613}}},{"id":{"replica_id":0,"value":190},"start":9819,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":616}}},{"id":{"replica_id":0,"value":191},"start":25626,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":612}}}],"summary":"Code Analysis Using Embeddings","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}],"thought_process_output_sections":[{"range":{"start":3346,"end":5267}},{"range":{"start":9819,"end":10182}}]}